{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar9nKDQgmPav"
      },
      "source": [
        "1.Submit a Google Colab notebook containing your completed code and experimentation results.\n",
        "\n",
        "2.Include comments and explanations in your code to help understand the implemented logic.\n",
        "\n",
        "**Additional Notes:**\n",
        "*   Ensure that the notebook runs successfully in Google Colab.\n",
        "*   Document any issues encountered during experimentation and how you addressed them.\n",
        "\n",
        "**Grading:**\n",
        "*   Each task will be graded out of the specified points.\n",
        "*   Points will be awarded for correctness, clarity of code, thorough experimentation, and insightful analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoUu86p1Or1n"
      },
      "source": [
        "# Prediction-Based Word Vectors\n",
        "\n",
        "more recently prediction-based word vectors have demonstrated better performance, such as word2vec and GloVe (which also utilizes the benefit of counts). Here, we shall explore the embeddings produced by GloVe.\n",
        "\n",
        "Then run the following cells to load the GloVe vectors into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvpYg_7pODYJ",
        "outputId": "e1834343-7ca2-452f-aa87-2c66ef5181fd"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 252.1/252.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "import pprint\n",
        "wv_from_bin = api.load(\"glove-wiki-gigaword-200\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFfCOLUsSSuS"
      },
      "source": [
        "### Words with Multiple Meanings\n",
        "Polysemes and homonyms are words that have more than one meaning (see this [wiki page](https://en.wikipedia.org/wiki/Polysemy) to learn more about the difference between polysemes and homonyms ). Find a word with *at least two different meanings* such that the top-10 most similar words (according to cosine similarity) contain related words from *both* meanings. For example, \"leaves\" has both \"go_away\" and \"a_structure_of_a_plant\" meaning in the top 10, and \"scoop\" has both \"handed_waffle_cone\" and \"lowdown\". You will probably need to try several polysemous or homonymic words before you find one.\n",
        "\n",
        "Please state the word you discover and the multiple meanings that occur in the top 10. Why do you think many of the polysemous or homonymic words you tried didn't work (i.e. the top-10 most similar words only contain **one** of the meanings of the words)?\n",
        "\n",
        "**Note**: You should use the `wv_from_bin.most_similar(word)` function to get the top 10 similar words. This function ranks all other words in the vocabulary with respect to their cosine similarity to the given word. For further assistance, please check the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.most_similar)__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZAr09U-xSSuT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd8061de-9e8d-4c63-b03c-a65d1bc869c3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('mice', 0.6580958962440491),\n",
              " ('keyboard', 0.5548278093338013),\n",
              " ('rat', 0.5433949828147888),\n",
              " ('rabbit', 0.5192376971244812),\n",
              " ('cat', 0.5077415704727173),\n",
              " ('cursor', 0.5058691501617432),\n",
              " ('trackball', 0.5048902630805969),\n",
              " ('joystick', 0.49841049313545227),\n",
              " ('mickey', 0.47242844104766846),\n",
              " ('clicks', 0.4722806215286255)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "wv_from_bin.most_similar('mouse')[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdQ018tjSSuT"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "I have tried different words: bat, bank, saw, rock, ... but any of them worked except for mouse.\n",
        "\n",
        "The alogithm chosen here is GloVe algorithm, which is a context-agnostic model that assigns a unique vector to each word, which remains static regardless of the word's varying context in different sentences. This means that it does not dynamically adapt to different usages of the same word, leading to a representation that cannot differentiate between the multiple meanings of polysemous words like \"bank\".\n",
        "\n",
        "In the case of \"bank,\" GloVe creates a singular, averaged representation that amalgamates all the contexts in which the word appears, resulting in a generalized semantic representation. This generalized representation may not capture the specific meaning of \"bank\" in a given context, such as \"river bank\" or \"financial bank.\" Instead, it may only capture the broader meaning of the word, which in this case is related to the economical meaning. This explanation holds true for other words I listed above, as well.\n",
        "\n",
        "But for the case of \"mouse\", it seems that there have been a good balance of both meanings in the dataset the GloVe was trained on and so we can see similar words to both of these meanings in the outputs. For example: mouse -> rat (for animal) and mouse -> keyboard (an electrical equipment)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfeW-eK9SSuU"
      },
      "source": [
        "### Synonyms & Antonyms\n",
        "\n",
        "When considering Cosine Similarity, it's often more convenient to think of Cosine Distance, which is simply 1 - Cosine Similarity.\n",
        "\n",
        "Find three words $(w_1,w_2,w_3)$ where $w_1$ and $w_2$ are synonyms and $w_1$ and $w_3$ are antonyms, but Cosine Distance $(w_1,w_3) <$ Cosine Distance $(w_1,w_2)$.\n",
        "\n",
        "As an example, $w_1$=\"happy\" is closer to $w_3$=\"sad\" than to $w_2$=\"cheerful\". Please find a different example that satisfies the above. Once you have found your example, please give a possible explanation for why this counter-intuitive result may have happened.\n",
        "\n",
        "You should use the the `wv_from_bin.distance(w1, w2)` function here in order to compute the cosine distance between two words. Please see the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.FastTextKeyedVectors.distance)__ for further assistance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bwlpPjpHSSuV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94dc81b5-4d58-4f54-a31e-72dcffe23378"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synonyms glad, joyous have cosine distance: 0.7180908620357513\n",
            "Antonyms glad, sad have cosine distance: 0.49190449714660645\n"
          ]
        }
      ],
      "source": [
        "w1 ='glad'\n",
        "w2 ='joyous'\n",
        "w3 ='sad'\n",
        "w1_w2_dist = wv_from_bin.distance(w1, w2)\n",
        "w1_w3_dist = wv_from_bin.distance(w1, w3)\n",
        "\n",
        "print(\"Synonyms {}, {} have cosine distance: {}\".format(w1, w2, w1_w2_dist))\n",
        "print(\"Antonyms {}, {} have cosine distance: {}\".format(w1, w3, w1_w3_dist))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeIHjTFMSSuV"
      },
      "source": [
        "### SOLUTION\n",
        "A possible explanation for this counter-intuitive result, where an antonym is closer to a word than a synonym in the embedding space, could be due to the way word embeddings are trained. GloVe embeddings are trained on co-occurrence statistics from a corpus. Words that often appear in similar contexts will be closer in the embedding space. Antonyms often appear in similar contexts because they are used in comparable situations, even though they have opposite meanings. For example, \"glad\" and \"sad\" might both be used frequently in contexts discussing emotion of people which could make their embeddings closer than expected.\n",
        "\n",
        "Another contributing factor could be polysemy, where a word has multiple meanings. If \"glad\" is used in different contexts that align more closely with \"sad\" than with some of the contexts of \"joyous\", this could also bring \"glad\" and \"sad\" closer together in the embedding space.\n",
        "\n",
        "Lastly, the training data itself and the frequency of certain words can influence the resulting embeddings. If the corpus used to train the embeddings has more instances where \"glad\" and \"sad\" are used together compared to \"glad\" and \"joyous\", this could also result in a smaller cosine distance between \"glad\" and \"sad\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxIDq26zSSuW"
      },
      "source": [
        "### Analogies with Word Vectors\n",
        "Word vectors have been shown to *sometimes* exhibit the ability to solve analogies.\n",
        "\n",
        "As an example, for the analogy \"man : grandfather :: woman : x\" (read: man is to grandfather as woman is to x), what is x?\n",
        "\n",
        "In the cell below, we show you how to use word vectors to find x using the `most_similar` function from the __[GenSim documentation](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar)__. The function finds words that are most similar to the words in the `positive` list and most dissimilar from the words in the `negative` list. The answer to the analogy will have the highest cosine similarity (largest returned numerical value)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "u0pC7H4VSSuY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d9372ee-34d4-456d-8a99-4225bd499802"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('grandmother', 0.7608445286750793),\n",
            " ('granddaughter', 0.7200808525085449),\n",
            " ('daughter', 0.7168302536010742),\n",
            " ('mother', 0.7151536345481873),\n",
            " ('niece', 0.7005682587623596),\n",
            " ('father', 0.6659887433052063),\n",
            " ('aunt', 0.6623408794403076),\n",
            " ('grandson', 0.6618767976760864),\n",
            " ('grandparents', 0.644661009311676),\n",
            " ('wife', 0.6445354223251343)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to answer the analogy -- man : grandfather :: woman : x\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVv8I9WwSSuZ"
      },
      "source": [
        "Let $m$, $g$, $w$, and $x$ denote the word vectors for `man`, `grandfather`, `woman`, and the answer, respectively. Using **only** vectors $m$, $g$, $w$, and the vector arithmetic operators $+$ and $-$ in your answer, to what expression are we maximizing $x$'s cosine similarity?\n",
        "\n",
        "Hint: Recall that word vectors are simply multi-dimensional vectors that represent a word. It might help to draw out a 2D example using arbitrary locations of each vector. Where would `man` and `woman` lie in the coordinate plane relative to `grandfather` and the answer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlUKBqtHSSuZ"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "The mathematical expression is as follows:\n",
        "\n",
        "> x = w + g - m\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rRgMca9SSua"
      },
      "source": [
        "### Finding Analogies\n",
        "a. For the previous example, it's clear that \"grandmother\" completes the analogy. But give an intuitive explanation as to why the `most_similar` function gives us words like \"granddaughter\", \"daughter\", or \"mother?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgYQXazQSSua"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "The most similar words to grandmother are the words \"granddaughter\", \"daughter\" and \"mother\" as they are all part of close members in a family and so more likely to appear in most texts together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9aAUXEISSub"
      },
      "source": [
        "b. Find an example of analogy that holds according to these vectors (i.e. the intended word is ranked top). In your solution please state the full analogy in the form x:y :: a:b. If you believe the analogy is complicated, explain why the analogy holds in one or two sentences.\n",
        "\n",
        "**Note**: You may have to try many analogies to find one that works!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dhzQJMYYVSjf"
      },
      "outputs": [],
      "source": [
        "x, y, a, b = 'son', 'daughter', 'prince', 'princess'\n",
        "assert wv_from_bin.most_similar(positive=[a, y], negative=[x])[0][0] == b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3QlPqAwSSub"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "In the sample generated we have the analogy son:daughter :: prince:princess. As prince is what we call the son of a king, princess is the daughter of him."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwgcEywwSSuc"
      },
      "source": [
        "### Incorrect Analogy\n",
        "a. Below, we expect to see the intended analogy \"hand : glove :: foot : **sock**\", but we see an unexpected result instead. Give a potential reason as to why this particular analogy turned out the way it did?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m-ykWoJoSSuc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eebedacb-db9a-45a9-cf71-70bb732344e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('45,000-square', 0.4922032654285431),\n",
            " ('15,000-square', 0.4649604558944702),\n",
            " ('10,000-square', 0.4544755816459656),\n",
            " ('6,000-square', 0.44975775480270386),\n",
            " ('3,500-square', 0.444133460521698),\n",
            " ('700-square', 0.44257497787475586),\n",
            " ('50,000-square', 0.4356396794319153),\n",
            " ('3,000-square', 0.43486514687538147),\n",
            " ('30,000-square', 0.4330596923828125),\n",
            " ('footed', 0.43236875534057617)]\n"
          ]
        }
      ],
      "source": [
        "pprint.pprint(wv_from_bin.most_similar(positive=['foot', 'glove'], negative=['hand']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zn4ruS8MSSud"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "As we stated above, a polysomy word such that we have here \"foot\" may have similar words only to one of its meanings. And here it seems like the model was trained on the corpus with more data about measurements, in compared to contexts on body parts.\n",
        "\n",
        "Plus the fact that the context on which model was trained on, didn't have enough data about desired outcome, or maybe the dataset was just not well balanced and vocabulary was diverse.\n",
        "\n",
        "It is also possible that there were simply not enough examples of the relationship between \"foot\" and \"sock\" in the training data for the word vector model to recognize the analogy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1gHyZt0SSud"
      },
      "source": [
        "b. Find another example of analogy that does *not* hold according to these vectors. In your solution, state the intended analogy in the form x:y :: a:b, and state the **incorrect** value of b according to the word vectors (in the previous example, this would be **'45,000-square'**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D_rlci42XQTw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37060252-648d-45a4-d298-8769edd90b20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('priests', 0.6184417605400085),\n",
            " ('church', 0.5651061534881592),\n",
            " ('catholic', 0.5578794479370117),\n",
            " ('clergy', 0.5117530822753906),\n",
            " ('archdiocese', 0.5048693418502808),\n",
            " ('parishioners', 0.5043907761573792),\n",
            " ('nuns', 0.4974152743816376),\n",
            " ('outside', 0.46567755937576294),\n",
            " ('priesthood', 0.46334171295166016),\n",
            " ('ordained', 0.46231380105018616)]\n"
          ]
        }
      ],
      "source": [
        "x, y, a, b = 'ambassador', 'embassy', 'priest', 'church'\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[a, y], negative=[x]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4x0EHjeSSue"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "Intended Analogy: abassador:embassy :: priest:church\n",
        "In fact each person is mapped to where he stays.\n",
        "\n",
        "False Analogy: priesets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvlycXN-SSuf"
      },
      "source": [
        "### Guided Analysis of Bias in Word Vectors\n",
        "\n",
        "It's important to be cognizant of the biases (gender, race, sexual orientation etc.) implicit in our word embeddings. Bias can be dangerous because it can reinforce stereotypes through applications that employ these models.\n",
        "\n",
        "Run the cell below, to examine (a) which terms are most similar to \"woman\" and \"profession\" and most dissimilar to \"man\", and (b) which terms are most similar to \"man\" and \"profession\" and most dissimilar to \"woman\". Point out the difference between the list of female-associated words and the list of male-associated words, and explain how it is reflecting gender bias."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XggWA4MhSSuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88452b4b-720c-4fa6-b18b-cac319513873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('reputation', 0.5250176787376404),\n",
            " ('professions', 0.5178037881851196),\n",
            " ('skill', 0.49046966433525085),\n",
            " ('skills', 0.49005505442619324),\n",
            " ('ethic', 0.4897659420967102),\n",
            " ('business', 0.4875852167606354),\n",
            " ('respected', 0.485920250415802),\n",
            " ('practice', 0.482104629278183),\n",
            " ('regarded', 0.4778572618961334),\n",
            " ('life', 0.4760662019252777)]\n",
            "\n",
            "[('professions', 0.5957457423210144),\n",
            " ('practitioner', 0.49884122610092163),\n",
            " ('teaching', 0.48292139172554016),\n",
            " ('nursing', 0.48211804032325745),\n",
            " ('vocation', 0.4788965880870819),\n",
            " ('teacher', 0.47160351276397705),\n",
            " ('practicing', 0.46937814354896545),\n",
            " ('educator', 0.46524327993392944),\n",
            " ('physicians', 0.4628995358943939),\n",
            " ('professionals', 0.4601394236087799)]\n"
          ]
        }
      ],
      "source": [
        "# Run this cell\n",
        "# Here `positive` indicates the list of words to be similar to and `negative` indicates the list of words to be most dissimilar from.\n",
        "\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4g6KbsYSSuh"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "The first list, shows the words most similar to man and profession, which are some quality related words, while the 2nd list, is more consisted we can see name of jobs themselves. These jobs are related to education or caring, and so are connected with social sectors, with possibly lower rate of income.\n",
        "\n",
        "We can conclude that the word embedding model has leanrned a gender stereotype that men are more likely to possess these qualities than women and that women have some specific jobs that are defined for them. So if we want to use this type of embedding for an allocation task, then the output won't be good at all and will result in neglecting women for higer-paid and professional jobs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxJmnS6lSSui"
      },
      "source": [
        "### Independent Analysis of Bias in Word Vectors\n",
        "\n",
        "Use the `most_similar` function to find another pair of analogies that demonstrates some bias is exhibited by the vectors. Please briefly explain the example of bias that you discover."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PZoDheIfSSui",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4e2b12c-cd60-4dd1-aa7c-0b35497d69f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('character', 0.5096344351768494),\n",
            " ('personalities', 0.4690902531147003),\n",
            " ('mind', 0.4690796434879303),\n",
            " ('show', 0.4587042033672333),\n",
            " ('sense', 0.45704707503318787),\n",
            " ('reality', 0.4549425542354584),\n",
            " ('attitude', 0.44896262884140015),\n",
            " ('persona', 0.4450281262397766),\n",
            " ('comedian', 0.43983012437820435),\n",
            " ('kind', 0.435565710067749)]\n",
            "\n",
            "[('personalities', 0.5307511687278748),\n",
            " ('traits', 0.49145638942718506),\n",
            " ('temperament', 0.46246960759162903),\n",
            " ('mbeki', 0.46054622530937195),\n",
            " ('schizotypal', 0.4567224681377411),\n",
            " ('trait', 0.4478680193424225),\n",
            " ('africa', 0.4334438443183899),\n",
            " ('antisocial', 0.43069028854370117),\n",
            " ('mandela', 0.4262062609195709),\n",
            " ('dynamic', 0.40384382009506226)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "A = 'american'\n",
        "B = 'african'\n",
        "word = 'personality'\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[A, word], negative=[B]))\n",
        "print()\n",
        "pprint.pprint(wv_from_bin.most_similar(positive=[B, word], negative=[A]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGOlmtJoSSuj"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "Here we compared \"american\" and \"african\" people, in the field of personality. As you can observe, there are some good personalities like \"kind\" or \"comedian\" which are considered to be connected with American people, while some personalies like \"schizotypal\" or \"antisocial\" are connected with African people. These two personalities are not bad inherently, but when we give these two personalities to a group of people, the opinion we want to give based on the ouptut of model would become biased and would be considered that they may have disorder of such personalities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK2XVWzmSSuk"
      },
      "source": [
        "### Thinking About Bias\n",
        "\n",
        "a. Give one explanation of how bias gets into the word vectors. Briefly describe a real-world example that demonstrates this source of bias."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19pM85fCSSuk"
      },
      "source": [
        "### SOLUTION\n",
        "\n",
        "The most likely cause for such bias, is the dataset that we used for training. Data in training corpus may have had such bias in itself and thus the model leanred these implicit biases and stereotypes.\n",
        "\n",
        "And it also turns out that embeddings don't just reflect the statistics of their input, but also amplify bias. This fact was concluded serveral papers like [(Zhao et al. 2017)](https://doi.org/10.18653/v1/D17-1323) which is one example of the fact that gendered terms become more gendered in embedding space than they were in the input text statistics.\n",
        "\n",
        "Another real world example is from the paper [(Bolukbasi et al. 2016)](https://arxiv.org/abs/1607.06520) that finds that the closest word occupation to 'computer programmer' - 'man' + 'woman' in word2vec embeddings tained on news text is 'homemaker', and that the embeddings similarly suggest the analogy 'father':'docter' :: 'mother':'nurse'. This kind of analogy leaned by embedding model, may cause allocational harm, which is referred to when a system allocates resources (jobs or credit) unfairly to different groups.\n",
        "\n",
        "These explanations were taken from reference book (Jurafsky)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILYqJZ7ASSul"
      },
      "source": [
        "b. What is one method you can use to mitigate bias exhibited by word vectors?  Briefly describe a real-world example that demonstrates this method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnJaAB7mSSul"
      },
      "source": [
        "\n",
        "### SOLUTION\n",
        "\n",
        "A real-world example that demonstrates this method is the work by [Bolukbasi et al. (2016b)](https://arxiv.org/abs/1607.06520), who proposed a post-processing debiasing method, known as \"Hard Debiasing\" that makes changes to the word vectors they achieved from the news corpus described above, in order to reduce gender bias for all words that are not inherently gendered.\n",
        "\n",
        "They achieved this by first identifying gender-specific words and concepts by looking at the cosine similarity between pairs of words and analyzing which pairs showed the largest differences between male and female pronouns. Then\n",
        "They zeroed the gender projection of each word on a predefined gender direction and ensured that all neutral words are equally close to each of the two gendered words in an equality set.\n",
        "\n",
        "So by doing this, they developed a transformation of the embedding space that removes gender stereotypes but preserves definitional gender. However, although these sorts of debiasing may reduce bias in embeddings, they do not eliminate it [(Gonen and Goldberg, 2019)](https://aclanthology.org/W19-3621/), and this remains an open problem.\n",
        "\n",
        "This part answered using ChatGPT and reference book (Jurafsky)."
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUiB9GhqAcmE"
      },
      "source": [
        "# Homework: Implementing and Training a Masked Language Model (MLM)\n",
        "\n",
        "## Overview\n",
        "In this assignment, you will implement a basic Masked Language Model (MLM) from scratch using PyTorch. This exercise will help you understand the architecture and training process of MLMs, which are crucial for many NLP tasks. You will use the `bookcorpus` dataset from the Hugging Face `datasets` library for training.\n",
        "\n",
        "## Objectives\n",
        "- Implement a Transformer-based MLM.\n",
        "- Preprocess and prepare a dataset for MLM training.\n",
        "- Train your model on the `bookcorpus` dataset.\n",
        "- Evaluate the model on masked sentence predictions.\n",
        "\n",
        "## Getting Started\n",
        "First, let's import all necessary libraries and set up our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhTgyiHRMjKm",
        "outputId": "f6e82bd5-9254-4723-b745-ec4e21e1bfa1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Collecting huggingface-hub>=0.21.2 (from datasets)\n",
            "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 huggingface-hub-0.23.0 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7Yd4SioAKnl"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Check device availability\n",
        "\n",
        "# TODO :\n",
        "# Check whether the cuda is available (if not assign cpu)\n",
        "device = None\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGRFSqBnBZwS"
      },
      "source": [
        "## Dataset Preparation\n",
        "For this assignment, we will use the `bookcorpus` dataset available through Hugging Face's datasets library. You need to load the dataset, preprocess it for MLM, and create a PyTorch dataset class.\n",
        "\n",
        "### Load the Dataset\n",
        "First, load the `bookcorpus` dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r72aUoBtBaoD"
      },
      "outputs": [],
      "source": [
        "# TO DO:\n",
        "# load bookcorpus dataset\n",
        "# we just need train split of it.\n",
        "dataset = None\n",
        "print(\"Dataset loaded successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rJvUO9UB3on"
      },
      "outputs": [],
      "source": [
        "# Shuffle the dataset and reduce its size\n",
        "dataset = dataset.shuffle(seed=42).select(range(100000))\n",
        "print(\"Reduced dataset loaded successfully. Total samples:\", len(dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIugmLM3B69y"
      },
      "source": [
        "### Preprocess the Data\n",
        "To train our MLM, we need to preprocess our textual data appropriately. This preprocessing involves tokenizing our text into tokens that the MLM can understand and randomly masking some of these tokens to create input-output pairs for our model to learn from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGBnh3WpB7jQ"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvM-rhA7CEbw"
      },
      "source": [
        "### TextDataset Class\n",
        "Below is the `TextDataset` class where you will implement the tokenization, and specifically, the logic to create the masking array. The class should take a list of texts and a tokenizer as input. It should tokenize the texts, apply the masking randomly to 15% of the tokens, and prepare the input and label pairs for the MLM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G85WMXLpCGic"
      },
      "outputs": [],
      "source": [
        "# TextDataset Class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataset, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset = dataset\n",
        "        self.max_len = max_len\n",
        "        self.inputs = []\n",
        "\n",
        "        for index, text in tqdm(enumerate(self.dataset['text']), total = len(self.dataset['text'])):\n",
        "            # clean the text\n",
        "            text = preprocess_text(text)\n",
        "\n",
        "            # Tokenize the text\n",
        "            tokenized_text = self.tokenizer.encode_plus(\n",
        "                text,\n",
        "                max_length=self.max_len,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "            input_ids = tokenized_text['input_ids'].squeeze(0)\n",
        "            labels = input_ids.clone()\n",
        "\n",
        "            # Create random array to determine which tokens to mask\n",
        "            rand = torch.rand(input_ids.shape)\n",
        "\n",
        "            # To Do: Implement the masking logic here\n",
        "            # Create a mask array - mask 15% of tokens that are not special tokens\n",
        "            # special tokens are: Pad Token, CLS Token, SEP Token\n",
        "            # Your code here: mask_arr = ...\n",
        "\n",
        "            labels[~mask_arr] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "            # 80% of the time, we replace masked input tokens with the mask token\n",
        "            indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & mask_arr\n",
        "            input_ids[indices_replaced] = self.tokenizer.mask_token_id\n",
        "\n",
        "            # 10% of the time, replace masked input tokens with random words\n",
        "            indices_random = torch.bernoulli(torch.full(labels.shape, 0.1)).bool() & mask_arr & ~indices_replaced\n",
        "            random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "            input_ids[indices_random] = random_words[indices_random]\n",
        "\n",
        "            # Add the prepared inputs and labels to the list\n",
        "            self.inputs.append({\"input_ids\": input_ids, \"labels\": labels})\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LNQPANEEC6K"
      },
      "source": [
        "## Understanding the Masking Strategy in Masked Language Models\n",
        "\n",
        "### Question Overview\n",
        "\n",
        "In the training process of Masked Language Models (MLMs) such as BERT, a specific strategy for masking tokens is commonly employed:\n",
        "- **80%** of the masked tokens are replaced with the `[MASK]` token.\n",
        "- **10%** are replaced with random words.\n",
        "- **10%** are left unchanged.\n",
        "\n",
        "This methodical approach to token masking plays a crucial role in how the model learns during the pre-training phase.\n",
        "\n",
        "### Detailed Questions\n",
        "\n",
        "Please provide a comprehensive explanation addressing the rationale behind this masking strategy. Your response should cover the following aspects:\n",
        "\n",
        "1. **80% Masked with `[MASK]` Token:**\n",
        "   - **Why are 80% of the masked tokens replaced with the `[MASK]` token?**\n",
        "   - Discuss how this percentage influences the model's focus during training and its ability to learn contextual information from surrounding tokens.\n",
        "\n",
        "2. **10% Replaced with Random Words:**\n",
        "   - **Why are 10% of the masked tokens randomly replaced with other words from the vocabulary?**\n",
        "   - Analyze the impact of this strategy on the model's robustness and its handling of unexpected or novel input during real-world applications.\n",
        "\n",
        "3. **10% Left Unchanged:**\n",
        "   - **Why are the remaining 10% of the masked tokens left as is, unchanged?**\n",
        "   - Consider how leaving some masked tokens unchanged might help the model generalize better and avoid overfitting to the `[MASK]` token specifically.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80mYw2VKEqNb"
      },
      "source": [
        "## Transformer Model Components Explanation\n",
        "\n",
        "In this section of the assignment, you will implement the core components of a Transformer model tailored for Masked Language Modeling (MLM). Understanding the functionality of each component is crucial for your implementation. Below, we detail the roles and responsibilities of each component within the Transformer architecture.\n",
        "\n",
        "### Transformer Block\n",
        "\n",
        "The Transformer Block is the fundamental building unit of a Transformer model. Each block consists of two main parts: a multi-head self-attention mechanism and a position-wise feed-forward network.\n",
        "\n",
        "- **Multi-Head Self-Attention:** This component allows the model to dynamically focus on different parts of the input sequence, learning nuanced dependencies between words regardless of their positional distance from each other. It helps the model understand the context and relationships within the text.\n",
        "- **Feed-Forward Network:** Following the attention mechanism, each position is passed through the same feed-forward network independently. This network transforms the attended features to help in predicting the correct output tokens.\n",
        "- **Normalization and Dropout:** Each sub-layer (attention and feed-forward) in the block includes a residual connection followed by layer normalization. Dropout is applied for regularization to prevent overfitting.\n",
        "\n",
        "### Encoder\n",
        "\n",
        "The Encoder aggregates multiple Transformer Blocks to process the input tokens. Its main responsibilities include:\n",
        "\n",
        "- **Embedding Inputs:** Initially, input tokens are converted into embeddings that represent them in a continuous vector space. Positional embeddings are added to these token embeddings to retain positional information.\n",
        "- **Processing through Transformer Blocks:** The embedded input tokens are then sequentially passed through multiple Transformer Blocks. Each block processes the input and passes its output to the next block, iteratively refining the representations.\n",
        "- **Output:** The final output of the Encoder is a sequence of vectors, where each vector is a rich representation of the corresponding input token, considering the entire input sequence context.\n",
        "\n",
        "### MLM Model\n",
        "\n",
        "The MLM Model is the overarching architecture that utilizes the Encoder for the MLM task. It is specifically designed for predicting masked tokens, replicating the pre-training objective of models like BERT.\n",
        "\n",
        "- **Encoder Utilization:** The MLM Model embeds the input tokens and passes them through the Encoder to obtain contextualized token representations.\n",
        "- **Prediction Layer:** On top of the Encoder's output, a linear layer is used to map the high-dimensional token representations back to the vocabulary space. This setup predicts the original token from its masked version or context.\n",
        "- **Training Objective:** The primary goal during training is to minimize the prediction error of the masked tokens, encouraging the model to understand and generate language effectively.\n",
        "\n",
        "### Implementation Notes\n",
        "\n",
        "As you implement these components:\n",
        "- Focus on how each part contributes to handling and transforming the input data into useful representations.\n",
        "- Consider the flow of data through the model and how each component’s output serves as input to the next.\n",
        "- Ensure your implementation supports backpropagation, as this will be crucial for training the model.\n",
        "\n",
        "In the upcoming coding tasks, you will implement these components based on the provided templates and hints. This hands-on experience will deepen your understanding of how modern NLP models leverage Transformer architectures for complex language understanding tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qrEku6mEEKg"
      },
      "outputs": [],
      "source": [
        "# Transformer Block\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "        # Feed-forward layer\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
        "            nn.ReLU(),\n",
        "            # To Do: Add the final linear layer (hint: the output size should be the same as embed_size)\n",
        "\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, value, key, query, mask):\n",
        "        attention = self.attention(query, key, value, attn_mask=mask)[0]\n",
        "        x = self.dropout(self.norm1(attention + query))\n",
        "        forward = self.feed_forward(x)\n",
        "        out = self.dropout(self.norm2(forward + x))\n",
        "        return out\n",
        "\n",
        "# Encoder\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        N, seq_length = x.shape\n",
        "        positions = torch.arange(0, seq_length).unsqueeze(0).repeat(N, 1).to(self.device)\n",
        "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
        "\n",
        "        for layer in self.layers:\n",
        "            out = layer(out, out, out, mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "# MLM Model\n",
        "class MLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, device, max_length):\n",
        "        super(MLM, self).__init__()\n",
        "        self.encoder = Encoder(vocab_size, embed_size, num_layers, heads, device, forward_expansion, dropout, max_length)\n",
        "        # To Do: Initialize an output layer for predicting masked tokens (hint: use a linear layer)\n",
        "        self.fc_out = None\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        out = self.encoder(x, mask)\n",
        "        # To Do: Apply the output layer to 'out' (hint: remember to reshape if needed before applying it)\n",
        "        out = None\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD5xIba-JgYL"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Training the MLM is a crucial step where the model learns to predict the masked tokens based on the context provided by the surrounding words. This process involves passing batches of preprocessed data through the model, calculating the loss, and updating the model parameters to minimize this loss.\n",
        "\n",
        "### Setting Up the Training Loop\n",
        "\n",
        "The objective of the training loop is to iteratively improve the model's predictions. During each epoch, the model will process all batches of data, calculate the loss for each batch, and update its weights. You will complete parts of the training function to ensure proper loss calculation and parameter optimization.\n",
        "\n",
        "**To Do:**\n",
        "- Complete the loss calculation using the appropriate loss function.\n",
        "- Implement the steps for backpropagation and updating model parameters.\n",
        "- Monitor and print the average loss after each epoch to track the training progress.\n",
        "\n",
        "In the next section, you will find a partially completed training function. Fill in the missing parts as instructed to complete the training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBJd4nRPIXwC"
      },
      "outputs": [],
      "source": [
        "def train(model, data_loader, optimizer, device, epochs=10):\n",
        "    model = model.to(device)\n",
        "    model.train()  # Set the model to training mode\n",
        "    total_loss = 0\n",
        "\n",
        "    # To Do : Initialize the loss function with ignore_index set to -100\n",
        "    # Hint: CrossEntropyLoss might be appropriate here.\n",
        "    loss_function = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
        "        for i, batch in progress_bar:\n",
        "            input_ids, labels = batch['input_ids'].to(device), batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, None)  # Assuming mask=None for simplicity\n",
        "\n",
        "            # To Do: Calculate loss. Attention to shapes of the outputs and labels\n",
        "            loss = None\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()  # Clear existing gradients\n",
        "            loss.backward()  # Compute gradient of loss with respect to model parameters\n",
        "            optimizer.step()  # Update model parameters\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        average_loss = total_loss / len(data_loader)\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Average Loss: {average_loss}\")\n",
        "        total_loss = 0  # Reset total loss for the next epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfcZA_JAKBD1"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "Evaluating the performance of your Masked Language Model (MLM) is essential to understand how well it has learned to predict masked tokens. This evaluation typically involves using the model to predict tokens in place of `[MASK]` and comparing these predictions to the actual tokens. This step is crucial for assessing the model's ability to generalize to unseen data and for verifying its learning efficacy.\n",
        "\n",
        "### Setting Up the Evaluation Function\n",
        "\n",
        "The evaluation function will test the model's ability to fill in masked tokens correctly within given text examples. You will complete parts of this function to ensure the model can perform forward passes on input data and process the output to generate human-readable predictions.\n",
        "\n",
        "**To Do:**\n",
        "- Implement the logic to convert model predictions to token IDs.\n",
        "- Translate these token IDs back to words using the tokenizer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixGnC8fRKCwb"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, text, tokenizer, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "\n",
        "    # Tokenize the input text where `[MASK]` is the token to predict\n",
        "    tokenized_input = tokenizer.encode_plus(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "    input_ids = tokenized_input['input_ids'].to(device)\n",
        "\n",
        "    # Identify the position of the `[MASK]` token\n",
        "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    with torch.no_grad():  # No need to calculate gradients for evaluation\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids, None)\n",
        "\n",
        "        # Get the logits and find the 10 tokens with the highest probability at the mask position\n",
        "        # To Do: Implement the logic to extract the 10 token IDs with the highest probability for the mask position\n",
        "        # use torch.topk function for mask token logits\n",
        "        mask_token_logits = None\n",
        "        top_k_probabilities, top_k_indices = None  \n",
        "\n",
        "        # Convert the predicted token IDs to words using the tokenizer\n",
        "        top_k_tokens = [tokenizer.convert_ids_to_tokens(indices.cpu().numpy()) for indices in top_k_indices]\n",
        "\n",
        "    return top_k_tokens\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3cCR132LOXo"
      },
      "source": [
        "## Starting the MLM Training and Evaluation with bookcorpus Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0lPeJamMJXR"
      },
      "outputs": [],
      "source": [
        "# To Do:  Load the BERT tokenizer (bert base uncased)\n",
        "tokenizer = None\n",
        "\n",
        "text_dataset = TextDataset(dataset, tokenizer, max_len=512)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z28CGMnrDKX4"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(text_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPVK3puuDM5m"
      },
      "outputs": [],
      "source": [
        "# To Do: Initialize the MLM model with the following hyper-paramaters\n",
        "# vocab size = (comes from tokenizer)\n",
        "# embed size = 256\n",
        "# num of layers = 2\n",
        "# heads = 8\n",
        "# forward expansion = 4\n",
        "# dropout = 0.1\n",
        "# max_length = 512\n",
        "\n",
        "model = None\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVF4hcpjDinZ"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "train(model, data_loader, optimizer, device, epochs=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nb2KAgZHDkxr"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "test_sentences = [\"Hello, my name is [MASK].\", \"The capital of France is [MASK].\", \"I love to [MASK] a song.\", '[MASK]']\n",
        "for sentence in test_sentences:\n",
        "    predicted_masks = evaluate(model, sentence, tokenizer, device)\n",
        "    print(f\"Original: {sentence}\")\n",
        "    print(\"10 most probable words for Mask token: \", predicted_masks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvEww2VEDlf_"
      },
      "source": [
        "## Improving Model Performance\n",
        "\n",
        "### Evaluation Results\n",
        "As you can see, the output of the evaluation is quite poor. Why? Because we started training the MLM from scratch. If we want to achieve an acceptable performance similar to a pretrained BERT model, we need to perform several steps. \n",
        "### Question\n",
        "**What steps can you take to improve the performance of your Masked Language Model (MLM)?**\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
